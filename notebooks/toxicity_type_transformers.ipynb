{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if str(Path(\".\").absolute().parent) not in sys.path:\n",
    "    sys.path.append(str(Path(\".\").absolute().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Initialize the env vars\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.utils import (\n",
    "    compute_pos_weight,\n",
    "    download_dataset,\n",
    "    flatten\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading OLID-BR from Kaggle.\n",
      "Train shape: (4765, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_offensive</th>\n",
       "      <th>is_targeted</th>\n",
       "      <th>targeted_type</th>\n",
       "      <th>toxic_spans</th>\n",
       "      <th>health</th>\n",
       "      <th>ideology</th>\n",
       "      <th>insult</th>\n",
       "      <th>lgbtqphobia</th>\n",
       "      <th>other_lifestyle</th>\n",
       "      <th>physical_aspects</th>\n",
       "      <th>profanity_obscene</th>\n",
       "      <th>racism</th>\n",
       "      <th>religious_intolerance</th>\n",
       "      <th>sexism</th>\n",
       "      <th>xenophobia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>430b13705cf34e13b74bc999425187c3</td>\n",
       "      <td>USER USER é muito bom. USER ^^ E claro a equip...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c779826dc43f460cb18e8429ca443477</td>\n",
       "      <td>Pior do que adolescentezinhas de merda...são p...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e64148caa4474fc79298e01d0dda8f5e</td>\n",
       "      <td>USER Toma no cu é vitamina como tu e tua prima.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "      <td>[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cc66b54eeec24607a67e2259134a1cdd</td>\n",
       "      <td>Muito bom, pena a circunstâncias serem ruins, ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[119, 120, 121, 122, 123, 124, 125, 126, 127, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a3d7839456ae4258a70298fcf637952e</td>\n",
       "      <td>Podia ter beijo também, pra ver se o homofóbic...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  430b13705cf34e13b74bc999425187c3   \n",
       "1  c779826dc43f460cb18e8429ca443477   \n",
       "2  e64148caa4474fc79298e01d0dda8f5e   \n",
       "3  cc66b54eeec24607a67e2259134a1cdd   \n",
       "4  a3d7839456ae4258a70298fcf637952e   \n",
       "\n",
       "                                                text is_offensive is_targeted  \\\n",
       "0  USER USER é muito bom. USER ^^ E claro a equip...          NOT         UNT   \n",
       "1  Pior do que adolescentezinhas de merda...são p...          OFF         UNT   \n",
       "2    USER Toma no cu é vitamina como tu e tua prima.          OFF         TIN   \n",
       "3  Muito bom, pena a circunstâncias serem ruins, ...          OFF         UNT   \n",
       "4  Podia ter beijo também, pra ver se o homofóbic...          OFF         UNT   \n",
       "\n",
       "  targeted_type                                        toxic_spans  health  \\\n",
       "0           NaN                                                NaN   False   \n",
       "1           NaN  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   False   \n",
       "2           GRP  [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17...   False   \n",
       "3           NaN  [119, 120, 121, 122, 123, 124, 125, 126, 127, ...   False   \n",
       "4           NaN  [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...   False   \n",
       "\n",
       "   ideology  insult  lgbtqphobia  other_lifestyle  physical_aspects  \\\n",
       "0     False   False        False            False             False   \n",
       "1     False    True        False            False             False   \n",
       "2     False    True        False            False             False   \n",
       "3     False    True        False            False             False   \n",
       "4     False    True        False            False             False   \n",
       "\n",
       "   profanity_obscene  racism  religious_intolerance  sexism  xenophobia  \n",
       "0              False   False                  False   False       False  \n",
       "1               True   False                  False    True       False  \n",
       "2               True   False                  False   False       False  \n",
       "3              False   False                  False   False       False  \n",
       "4              False   False                  False   False       False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = download_dataset([\"train.csv\", \"test.csv\"])\n",
    "\n",
    "train_df = dataset[\"train.csv\"]\n",
    "test_df = dataset[\"test.csv\"]\n",
    "\n",
    "del dataset\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (1589, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_offensive</th>\n",
       "      <th>is_targeted</th>\n",
       "      <th>targeted_type</th>\n",
       "      <th>toxic_spans</th>\n",
       "      <th>health</th>\n",
       "      <th>ideology</th>\n",
       "      <th>insult</th>\n",
       "      <th>lgbtqphobia</th>\n",
       "      <th>other_lifestyle</th>\n",
       "      <th>physical_aspects</th>\n",
       "      <th>profanity_obscene</th>\n",
       "      <th>racism</th>\n",
       "      <th>religious_intolerance</th>\n",
       "      <th>sexism</th>\n",
       "      <th>xenophobia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>da19df36730945f08df3d09efa354876</td>\n",
       "      <td>USER Adorei o comercial também Jesus. Só achei...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 6...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80f1a8c981864887b13963fed1261acc</td>\n",
       "      <td>Cara isso foi muito babaca geral USER conhece ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2f67025f913e4a6292e3d000d9e2b5a8</td>\n",
       "      <td>Se vc for porco, folgado e relaxado, você não ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>738ccd4476784f47af3a5a6cfdda4695</td>\n",
       "      <td>Se fosse um sniper ia ser louco</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[26, 27, 28, 29, 30]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e0064da693bd4c9e90ce8e6db8bd3bbb</td>\n",
       "      <td>USER é o meu saco USER USER USER</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[13, 14, 15, 16]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  da19df36730945f08df3d09efa354876   \n",
       "1  80f1a8c981864887b13963fed1261acc   \n",
       "2  2f67025f913e4a6292e3d000d9e2b5a8   \n",
       "3  738ccd4476784f47af3a5a6cfdda4695   \n",
       "4  e0064da693bd4c9e90ce8e6db8bd3bbb   \n",
       "\n",
       "                                                text is_offensive is_targeted  \\\n",
       "0  USER Adorei o comercial também Jesus. Só achei...          OFF         UNT   \n",
       "1  Cara isso foi muito babaca geral USER conhece ...          OFF         TIN   \n",
       "2  Se vc for porco, folgado e relaxado, você não ...          OFF         UNT   \n",
       "3                    Se fosse um sniper ia ser louco          OFF         UNT   \n",
       "4                   USER é o meu saco USER USER USER          OFF         UNT   \n",
       "\n",
       "  targeted_type                                        toxic_spans  health  \\\n",
       "0           NaN  [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 6...   False   \n",
       "1           GRP  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   False   \n",
       "2           NaN  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   False   \n",
       "3           NaN                               [26, 27, 28, 29, 30]   False   \n",
       "4           NaN                                   [13, 14, 15, 16]   False   \n",
       "\n",
       "   ideology  insult  lgbtqphobia  other_lifestyle  physical_aspects  \\\n",
       "0     False    True        False            False             False   \n",
       "1     False    True        False            False             False   \n",
       "2     False    True        False            False             False   \n",
       "3     False    True        False            False              True   \n",
       "4     False    True        False            False             False   \n",
       "\n",
       "   profanity_obscene  racism  religious_intolerance  sexism  xenophobia  \n",
       "0               True   False                  False   False       False  \n",
       "1              False   False                  False   False       False  \n",
       "2              False   False                  False   False       False  \n",
       "3              False   False                  False   False       False  \n",
       "4               True   False                  False   False       False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Test shape: {test_df.shape}\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (4272, 16)\n",
      "Test shape: (1438, 16)\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess the dataset.\n",
    "\n",
    "    Args:\n",
    "    - df: The dataset to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    - The preprocessed dataset.\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        \n",
    "        # Filter only offensive comments\n",
    "        df = df[df[\"is_offensive\"] == \"OFF\"]\n",
    "\n",
    "        # Remove religious_intolerance that has only one  sample\n",
    "        if \"religious_intolerance\" in df.columns:\n",
    "            df.drop(\"religious_intolerance\", axis=1, inplace=True)\n",
    "\n",
    "        # Filter only offensive comments with at least one toxicity label\n",
    "        df = df.loc[df.select_dtypes(\"bool\").sum(axis=1).ge(1)]\n",
    "\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "toxicity_labels = [\n",
    "    \"health\",\n",
    "    \"ideology\",\n",
    "    \"insult\",\n",
    "    \"lgbtqphobia\",\n",
    "    \"other_lifestyle\",\n",
    "    \"physical_aspects\",\n",
    "    \"profanity_obscene\",\n",
    "    \"racism\",\n",
    "    \"sexism\",\n",
    "    \"xenophobia\"\n",
    "]\n",
    "\n",
    "train_df = preprocessing(train_df)\n",
    "test_df = preprocessing(test_df)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[\"text\"].values\n",
    "y_train = train_df[toxicity_labels].astype(int).values\n",
    "\n",
    "X_test = test_df[\"text\"].values\n",
    "y_test = test_df[toxicity_labels].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameters(max_seq_length=1084, model_name='neuralmind/bert-base-portuguese-cased', model_type='bert', num_train_epochs=1, num_train_epochs_per_child=3, batch_size=1, validation_split=0.2, learning_rate=2e-05, seed=1993)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Parameters:\n",
    "    max_seq_length: int = len(max(X_train, key=len))\n",
    "    model_name: str = \"neuralmind/bert-base-portuguese-cased\"\n",
    "    model_type: str = \"bert\"\n",
    "    num_train_epochs: int = 1\n",
    "    num_train_epochs_per_child: int = 3\n",
    "    batch_size: int = 1\n",
    "    validation_split: float = 0.2\n",
    "    learning_rate: float = 2e-5\n",
    "    seed: int = 1993\n",
    "\n",
    "params = Parameters()\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier', 'bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Any, Union\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    create_optimizer\n",
    ")\n",
    "    \n",
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    params.model_name,\n",
    "    num_labels=len(toxicity_labels),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label={k:v for k, v in enumerate(toxicity_labels)},\n",
    "    label2id={v:k for k, v in enumerate(toxicity_labels)}\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    params.model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001bd02a6ff047f0a8988f6dc1b1888b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3417\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 855\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_fn(examples, tokenizer, max_seq_length):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": X_train,\n",
    "        \"labels\": y_train\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    preprocess_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"max_seq_length\": params.max_seq_length\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset = dataset.train_test_split(\n",
    "    test_size=params.validation_split,\n",
    "    shuffle=True,\n",
    "    seed=params.seed\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",\n",
    "    max_length=params.max_seq_length,\n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=params.batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    dataset[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=params.batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(tf_train_set) // params.batch_size\n",
    "total_train_steps = int(batches_per_epoch * params.num_train_epochs)\n",
    "\n",
    "optimizer, lr_scheduler = create_optimizer(\n",
    "    init_lr=params.learning_rate,\n",
    "    num_train_steps=total_train_steps,\n",
    "    num_warmup_steps=0\n",
    ")\n",
    "\n",
    "# Define loss and metrics\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "in user code:\n\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1532, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 579, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 695, in apply_gradients\n        self._create_all_weights(var_list)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 959, in _create_all_weights\n        self._create_slots(var_list)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py\", line 128, in _create_slots\n        self.add_slot(var, \"v\")\n    File \"c:\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1050, in add_slot\n        weight = tf.Variable(\n    File \"c:\\Python310\\lib\\site-packages\\keras\\initializers\\initializers_v2.py\", line 171, in __call__\n        return tf.zeros(shape, dtype)\n\n    ResourceExhaustedError: {{function_node __wrapped__Fill_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[29794,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\trajano\\OneDrive - HP Inc\\Doug\\GitHub\\pucrs\\ToChiquinho\\notebooks\\toxicity_type_transformers.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/trajano/OneDrive%20-%20HP%20Inc/Doug/GitHub/pucrs/ToChiquinho/notebooks/toxicity_type_transformers.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/trajano/OneDrive%20-%20HP%20Inc/Doug/GitHub/pucrs/ToChiquinho/notebooks/toxicity_type_transformers.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     tf_train_set,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/trajano/OneDrive%20-%20HP%20Inc/Doug/GitHub/pucrs/ToChiquinho/notebooks/toxicity_type_transformers.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mtf_validation_set,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/trajano/OneDrive%20-%20HP%20Inc/Doug/GitHub/pucrs/ToChiquinho/notebooks/toxicity_type_transformers.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mnum_train_epochs,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/trajano/OneDrive%20-%20HP%20Inc/Doug/GitHub/pucrs/ToChiquinho/notebooks/toxicity_type_transformers.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/trajano/OneDrive%20-%20HP%20Inc/Doug/GitHub/pucrs/ToChiquinho/notebooks/toxicity_type_transformers.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# callbacks=[lr_scheduler]\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/trajano/OneDrive%20-%20HP%20Inc/Doug/GitHub/pucrs/ToChiquinho/notebooks/toxicity_type_transformers.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1233\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1233\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   1234\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1235\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: in user code:\n\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1532, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 579, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 695, in apply_gradients\n        self._create_all_weights(var_list)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 959, in _create_all_weights\n        self._create_slots(var_list)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py\", line 128, in _create_slots\n        self.add_slot(var, \"v\")\n    File \"c:\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1050, in add_slot\n        weight = tf.Variable(\n    File \"c:\\Python310\\lib\\site-packages\\keras\\initializers\\initializers_v2.py\", line 171, in __call__\n        return tf.zeros(shape, dtype)\n\n    ResourceExhaustedError: {{function_node __wrapped__Fill_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[29794,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    tf_train_set,\n",
    "    validation_data=tf_validation_set,\n",
    "    epochs=params.num_train_epochs,\n",
    "    batch_size=params.batch_size,\n",
    "    # callbacks=[lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToxicityTypeDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Dict, List, Union\n",
    "from sklearn.base import BaseEstimator\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    create_optimizer\n",
    ")\n",
    "\n",
    "\n",
    "def format_scores(scores: Dict[str, Dict[str, float]]) -> Dict[str, float | int]:\n",
    "    \"\"\"Format scores to be logged in mlflow.\n",
    "\n",
    "    Args:\n",
    "    - scores: classification report scores\n",
    "\n",
    "    Returns:\n",
    "    - formatted_scores: formatted scores\n",
    "    \"\"\"\n",
    "    # Flatten the score dict\n",
    "    scores = flatten(scores)\n",
    "\n",
    "    # Remove whitespaces and \"-\" from keys\n",
    "    scores = {k.replace(\" \", \"_\").replace(\"-\", \"_\"): v for k, v in scores.items()}\n",
    "\n",
    "    # Remove \"_support\" items\n",
    "    scores = {k: v for k, v in scores.items() if not k.endswith(\"_support\")}\n",
    "\n",
    "    return scores\n",
    "    \n",
    "\n",
    "class ToxicityTypeDetector(BaseEstimator):\n",
    "    def __init__(self,\n",
    "                 model_type: str = \"bert\",\n",
    "                 model_name: str = \"neuralmind/bert-base-portuguese-cased\",\n",
    "                 labels: List[str] = None,\n",
    "                 max_seq_length: int = 512,\n",
    "                 num_train_epochs: int = 30,\n",
    "                 batch_size: int = 8,\n",
    "                 validation_split: float = 0.2,\n",
    "                 learning_rate: float = 2e-5,\n",
    "                 use_cuda: bool = True,\n",
    "                 **kwargs):\n",
    "\n",
    "        self.model_type = model_type\n",
    "        self.model_name = model_name\n",
    "        self.labels = labels\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.num_train_epochs = num_train_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.learning_rate = learning_rate\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.model = self.init_model()\n",
    "        self.tokenizer = self.init_tokenizer()\n",
    "\n",
    "    def init_tokenizer(self) -> Any:\n",
    "        return BertTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            model_max_length=self.max_seq_length\n",
    "        )\n",
    "\n",
    "    def init_model(self) -> Any:\n",
    "        return TFBertForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=len(self.labels),\n",
    "            problem_type=\"multi_label_classification\",\n",
    "            id2label={k:v for k, v in enumerate(self.labels)},\n",
    "            label2id={v:k for k, v in enumerate(self.labels)}\n",
    "        )\n",
    "\n",
    "    def init_optimizer(self, total_train_steps: int) -> Any:\n",
    "        \"\"\"Initialize the optimizer.\n",
    "\n",
    "        Args:\n",
    "        - total_train_steps: The total number of training steps.\n",
    "\n",
    "        Returns:\n",
    "        - The optimizer.\n",
    "        - The learning rate scheduler.\n",
    "        \"\"\"\n",
    "        return create_optimizer(\n",
    "            init_lr=self.learning_rate,\n",
    "            num_train_steps=total_train_steps,\n",
    "            num_warmup_steps=0\n",
    "        )\n",
    "\n",
    "    def _tokenize(self, X: str) -> Any:\n",
    "        return self.tokenizer.encode_plus(\n",
    "            X,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "\n",
    "    def _predict(self, X: str) -> np.ndarray:\n",
    "        \"\"\"Predicts the toxicity type of a given text.\n",
    "\n",
    "        Args:\n",
    "        - X: The text to be predicted.\n",
    "        \n",
    "        Returns:\n",
    "        - Probabilities of each toxicity type.\n",
    "        \"\"\"\n",
    "        inputs = self._tokenize(X)\n",
    "\n",
    "        logits = self.model(**inputs).logits\n",
    "        probs = tf.nn.sigmoid(logits).numpy()[0]\n",
    "        return probs\n",
    "\n",
    "    def predict_proba(self, X: Union[str, List[str], np.ndarray]) -> List[List[float]]:\n",
    "        \"\"\"Predicts the toxicity types of a given text.\n",
    "\n",
    "        Args:\n",
    "        - X: The text or texts to be predicted.\n",
    "\n",
    "        Returns:\n",
    "        - A list with a dictionary of probabilities for each toxicity type.\n",
    "        \"\"\"\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = X.tolist()\n",
    "        elif isinstance(X, str):\n",
    "            X = [X]\n",
    "\n",
    "        response = []\n",
    "        for x in X:\n",
    "            probs = self._predict(x)\n",
    "            response.append(\n",
    "                {k: v for k, v in zip(self.labels, probs)}\n",
    "            )\n",
    "            \n",
    "        return response\n",
    "\n",
    "    def predict(self, X: Union[str, List[str], np.ndarray]) -> List[List[float]]:\n",
    "        \"\"\"Predicts the toxicity types of a given text.\n",
    "\n",
    "        Args:\n",
    "        - X: The text or texts to be predicted.\n",
    "\n",
    "        Returns:\n",
    "        - A list with a dictionary of predicted toxicity types.\n",
    "        \"\"\"\n",
    "        preds = self.predict_proba(X)\n",
    "        preds = [{k: 1 if v > 0.5 else 0 for k, v in pred.items()} for pred in preds]\n",
    "        return preds\n",
    "\n",
    "    def prepare_data(self, X: np.ndarray, y: Union[list, np.ndarray]) -> Any:\n",
    "        \"\"\"Prepares the data to be used in the model.\n",
    "\n",
    "        Args:\n",
    "        - X: The texts to be used in the model.\n",
    "        - y: The labels to be used in the model.\n",
    "\n",
    "        Returns:\n",
    "        - train_dataset: The train dataset.\n",
    "        - val_dataset: The validation dataset.\n",
    "        \"\"\"\n",
    "        def preprocess_fn(examples, tokenizer):\n",
    "            return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        if isinstance(y, list):\n",
    "            y = np.array(y)\n",
    "\n",
    "        dataset = Dataset.from_dict(\n",
    "            {\n",
    "                \"text\": X,\n",
    "                \"labels\": y\n",
    "            }\n",
    "        )\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            preprocess_fn,\n",
    "            batched=True,\n",
    "            fn_kwargs={\"tokenizer\": self.tokenizer}\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorWithPadding(\n",
    "            tokenizer=self.tokenizer,\n",
    "            return_tensors=\"tf\")\n",
    "\n",
    "        dataset = dataset.train_test_split(\n",
    "            test_size=self.validation_split,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # print(dataset)\n",
    "        \n",
    "        tf_train_set = self.model.prepare_tf_dataset(\n",
    "            dataset[\"train\"],\n",
    "            shuffle=True,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "\n",
    "        tf_validation_set = self.model.prepare_tf_dataset(\n",
    "            dataset[\"test\"],\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "\n",
    "        return tf_train_set, tf_validation_set\n",
    "\n",
    "    def fit(self,\n",
    "            X: List[str],\n",
    "            y: List[List[int]] | np.ndarray) -> None:\n",
    "        \"\"\"Fits the model.\n",
    "\n",
    "        Args:\n",
    "        - X: The texts to be used for training.\n",
    "        - y: The labels to be used for training.\n",
    "        \"\"\"\n",
    "        self.model = self.init_model()\n",
    "        \n",
    "        train_set, val_set = self.prepare_data(X, y)\n",
    "\n",
    "        batches_per_epoch = len(train_set) // self.batch_size\n",
    "        total_train_steps = int(batches_per_epoch * self.num_train_epochs)\n",
    "\n",
    "        optimizer, lr_scheduler = self.init_optimizer(total_train_steps)\n",
    "\n",
    "        # Define loss and metrics\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss,\n",
    "            metrics=[\n",
    "                tf.keras.metrics.BinaryAccuracy()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.model.fit(\n",
    "            train_set,\n",
    "            validation_data=val_set,\n",
    "            epochs=self.num_train_epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            # callbacks=[lr_scheduler]\n",
    "        )\n",
    "\n",
    "    def score(self, X, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = ToxicityTypeDetector(\n",
    "    labels=toxicity_labels,\n",
    "    **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForSequenceClassification\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(params[\"model_name\"])\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    params[\"model_name\"],\n",
    "    num_labels=len(toxicity_labels),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label={k:v for k, v in enumerate(toxicity_labels)},\n",
    "    label2id={v:k for k, v in enumerate(toxicity_labels)}\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\"Olá, meu cachorro é tão fofo\", return_tensors=\"tf\")\n",
    "\n",
    "logits = model(**inputs).logits\n",
    "\n",
    "tf.nn.sigmoid(logits).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

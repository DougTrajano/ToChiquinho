{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"ToChiquinho","text":"<p>ToChiquinho is a tool that helps you to detect toxicity in text in Brazilian Portuguese.</p> <p>It's based on several models trained on OLID-BR dataset.</p> <p>ToChiquinho provides several methods to detect toxicity in text such as:</p> <ul> <li>Toxic Comment Classification: Detects if the text is toxic or not.</li> <li>Toxicity Type Detection: Detects more than 10 types of toxicity in text.</li> <li>Toxicity Target Classification: Detects if the text is targeted or not.</li> <li>Toxicity Target Type Identification: Identifies the type of toxicity in a text (individual, group, or other).</li> <li>Toxic Spans Detection: Detects the spans of text that are toxic.</li> </ul> ToChiquinho's architecture"},{"location":"index.html#motivation","title":"Motivation","text":"<p>Today, we are living in a world where the internet is becoming more and more important in our lives.</p> <p>So, we want to ensure healthy online communication between people. You can expose your opinion, but it should be respectful to others.</p>"},{"location":"models/toxic_comment_classification.html","title":"Toxic Comment Classification","text":"<p>Toxic Comment Classification is a model that detects if the text is toxic or not.</p> <p>This BERT model is a fine-tuned version of neuralmind/bert-base-portuguese-cased on the OLID-BR dataset.</p>"},{"location":"models/toxic_comment_classification.html#overview","title":"Overview","text":"<p>Input: Text in Brazilian Portuguese</p> <p>Output: Binary classification (toxic or not toxic)</p>"},{"location":"models/toxic_comment_classification.html#usage","title":"Usage","text":"<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"dougtrajano/toxic-comment-classification\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"dougtrajano/toxic-comment-classification\")\n</code></pre>"},{"location":"models/toxic_comment_classification.html#limitations-and-bias","title":"Limitations and bias","text":"<p>The following factors may degrade the model\u2019s performance.</p> <p>Text Language:  The model was trained on Brazilian Portuguese texts, so it may not work well with Portuguese dialects.</p> <p>Text Origin: The model was trained on texts from social media and a few texts from other sources, so it may not work well on other types of texts.</p>"},{"location":"models/toxic_comment_classification.html#trade-offs","title":"Trade-offs","text":"<p>Sometimes models exhibit performance issues under particular circumstances. In this section, we'll discuss situations in which you might discover that the model performs less than optimally, and should plan accordingly.</p> <p>Text Length: The model was fine-tuned on texts with a word count between 1 and 178 words (average of 18 words). It may give poor results on texts with a word count outside this range.</p>"},{"location":"models/toxic_comment_classification.html#performance","title":"Performance","text":"<p>The model was evaluated on the test set of the OLID-BR dataset.</p> <p>Accuracy: 0.8578</p> <p>Precision: 0.8594</p> <p>Recall: 0.8578</p> <p>F1-Score: 0.8580</p> Class Precision Recall F1-Score Support <code>NOT-OFFENSIVE</code> 0.8886 0.8490 0.8683 1,775 <code>OFFENSIVE</code> 0.8233 0.8686 0.8453 1,438"},{"location":"models/toxic_comment_classification.html#training-procedure","title":"Training procedure","text":""},{"location":"models/toxic_comment_classification.html#training-hyperparameters","title":"Training hyperparameters","text":"<p>The following hyperparameters were used during training:</p> <ul> <li>learning_rate: 3.255788747459486e-05</li> <li>train_batch_size: 8</li> <li>eval_batch_size: 8</li> <li>seed: 1993</li> <li>optimizer: Adam with betas=(0.8445637934160373,0.8338816842140165) and epsilon=2.527092625455385e-08</li> <li>lr_scheduler_type: linear</li> <li>num_epochs: 30</li> <li>label_smoothing_factor: 0.07158711257743958</li> </ul>"},{"location":"models/toxic_comment_classification.html#framework-versions","title":"Framework versions","text":"<ul> <li>Transformers 4.26.0</li> <li>Pytorch 1.10.2+cu113</li> <li>Datasets 2.9.0</li> <li>Tokenizers 0.13.2</li> </ul>"},{"location":"models/toxic_comment_classification.html#provide-feedback","title":"Provide Feedback","text":"<p>If you have any feedback on this model, please open an issue on GitHub.</p>"},{"location":"models/toxic_spans_detection.html","title":"Toxic Spans Detection","text":"<p>Toxic Spans Detection is a model that detects toxic spans in a given toxic text.</p> <p>This BERT model is a fine-tuned version of neuralmind/bert-base-portuguese-cased on the OLID-BR dataset.</p>"},{"location":"models/toxic_spans_detection.html#overview","title":"Overview","text":"<p>Input: Text in Brazilian Portuguese</p> <p>Output: A list of integers representing the position of the toxic spans in the text</p>"},{"location":"models/toxic_spans_detection.html#usage","title":"Usage","text":"<p>Pending</p>"},{"location":"models/toxic_spans_detection.html#limitations-and-bias","title":"Limitations and bias","text":"<p>The following factors may degrade the model\u2019s performance.</p> <p>Text Language:  The model was trained on Brazilian Portuguese texts, so it may not work well with Portuguese dialects.</p> <p>Text Origin: The model was trained on texts from social media and a few texts from other sources, so it may not work well on other types of texts.</p>"},{"location":"models/toxic_spans_detection.html#trade-offs","title":"Trade-offs","text":"<p>Sometimes models exhibit performance issues under particular circumstances. In this section, we'll discuss situations in which you might discover that the model performs less than optimally, and should plan accordingly.</p> <p>Text Length: The model was fine-tuned on texts with a word count between 1 and 178 words (average of 18 words). It may give poor results on texts with a word count outside this range.</p>"},{"location":"models/toxic_spans_detection.html#performance","title":"Performance","text":"<p>The model was evaluated on the test set of the OLID-BR dataset.</p> <p>Precision: 0.6876</p> <p>Recall: 0.4918</p> <p>F1-Score: 0.5734</p>"},{"location":"models/toxic_spans_detection.html#training-procedure","title":"Training procedure","text":""},{"location":"models/toxic_spans_detection.html#training-hyperparameters","title":"Training hyperparameters","text":"<p>The following hyperparameters were used during training:</p> <ul> <li>learning_rate: 0.00038798590315954165</li> <li>dropout_rate: 0.3</li> <li>seed: 1993</li> <li>optimizer: Adam with betas=(0.9978242993498763,0.9988901284249041) and epsilon=3.12576102525027e-08</li> <li>lr_scheduler_type: linear</li> <li>num_epochs: 30</li> <li>weight_decay: 0.1</li> </ul>"},{"location":"models/toxic_spans_detection.html#framework-versions","title":"Framework versions","text":"<ul> <li>SpaCy 3.4.1</li> <li>SpaCy pt_core_news_lg model 3.4.0</li> <li>Datasets 2.9.0</li> </ul>"},{"location":"models/toxic_spans_detection.html#provide-feedback","title":"Provide Feedback","text":"<p>If you have any feedback on this model, please open an issue on GitHub.</p>"},{"location":"models/toxicity_target_classification.html","title":"Toxicity Target Classification","text":"<p>Toxicity Target Classification is a model that classifies if a given text is targeted or not.</p> <p>This BERT model is a fine-tuned version of neuralmind/bert-base-portuguese-cased on the OLID-BR dataset.</p>"},{"location":"models/toxicity_target_classification.html#overview","title":"Overview","text":"<p>Input: Text in Brazilian Portuguese</p> <p>Output: Binary classification (targeted or untargeted)</p>"},{"location":"models/toxicity_target_classification.html#usage","title":"Usage","text":"<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"dougtrajano/toxicity-target-classification\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"dougtrajano/toxicity-target-classification\")\n</code></pre>"},{"location":"models/toxicity_target_classification.html#limitations-and-bias","title":"Limitations and bias","text":"<p>The following factors may degrade the model\u2019s performance.</p> <p>Text Language:  The model was trained on Brazilian Portuguese texts, so it may not work well with Portuguese dialects.</p> <p>Text Origin: The model was trained on texts from social media and a few texts from other sources, so it may not work well on other types of texts.</p>"},{"location":"models/toxicity_target_classification.html#trade-offs","title":"Trade-offs","text":"<p>Sometimes models exhibit performance issues under particular circumstances. In this section, we'll discuss situations in which you might discover that the model performs less than optimally, and should plan accordingly.</p> <p>Text Length: The model was fine-tuned on texts with a word count between 1 and 178 words (average of 18 words). It may give poor results on texts with a word count outside this range.</p>"},{"location":"models/toxicity_target_classification.html#performance","title":"Performance","text":"<p>The model was evaluated on the test set of the OLID-BR dataset.</p> <p>Accuracy: 0.6864</p> <p>Precision: 0.6882</p> <p>Recall: 0.6864</p> <p>F1-Score: 0.6872</p> Class Precision Recall F1-Score Support <code>UNTARGETED</code> 0.4912 0.5011 0.4961 443 <code>TARGETED INSULT</code> 0.7759 0.7688 0.7723 995"},{"location":"models/toxicity_target_classification.html#training-procedure","title":"Training procedure","text":""},{"location":"models/toxicity_target_classification.html#training-hyperparameters","title":"Training hyperparameters","text":"<p>The following hyperparameters were used during training:</p> <ul> <li>learning_rate: 4.174021560583183e-05</li> <li>train_batch_size: 8</li> <li>eval_batch_size: 8</li> <li>seed: 1993</li> <li>optimizer: Adam with betas=(0.9360294728287728,0.9974781444436187) and epsilon=8.016624612627008e-07</li> <li>lr_scheduler_type: linear</li> <li>num_epochs: 30</li> <li>label_smoothing_factor: 0.09936835309930625</li> </ul>"},{"location":"models/toxicity_target_classification.html#framework-versions","title":"Framework versions","text":"<ul> <li>Transformers 4.26.0</li> <li>Pytorch 1.10.2+cu113</li> <li>Datasets 2.9.0</li> <li>Tokenizers 0.13.2</li> </ul>"},{"location":"models/toxicity_target_classification.html#provide-feedback","title":"Provide Feedback","text":"<p>If you have any feedback on this model, please open an issue on GitHub.</p>"},{"location":"models/toxicity_target_type_identification.html","title":"Toxicity Target Type Identification","text":"<p>Toxicity Target Type Identification is a model that classifies the type (individual, group, or other) of a given targeted text.</p> <p>This BERT model is a fine-tuned version of neuralmind/bert-base-portuguese-cased on the OLID-BR dataset.</p>"},{"location":"models/toxicity_target_type_identification.html#overview","title":"Overview","text":"<p>Input: Text in Brazilian Portuguese</p> <p>Output: Multiclass classification (individual, group, or other)</p>"},{"location":"models/toxicity_target_type_identification.html#usage","title":"Usage","text":"<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"dougtrajano/toxicity-target-type-identification\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"dougtrajano/toxicity-target-type-identification\")\n</code></pre>"},{"location":"models/toxicity_target_type_identification.html#limitations-and-bias","title":"Limitations and bias","text":"<p>The following factors may degrade the model\u2019s performance.</p> <p>Text Language:  The model was trained on Brazilian Portuguese texts, so it may not work well with Portuguese dialects.</p> <p>Text Origin: The model was trained on texts from social media and a few texts from other sources, so it may not work well on other types of texts.</p>"},{"location":"models/toxicity_target_type_identification.html#trade-offs","title":"Trade-offs","text":"<p>Sometimes models exhibit performance issues under particular circumstances. In this section, we'll discuss situations in which you might discover that the model performs less than optimally, and should plan accordingly.</p> <p>Text Length: The model was fine-tuned on texts with a word count between 1 and 178 words (average of 18 words). It may give poor results on texts with a word count outside this range.</p>"},{"location":"models/toxicity_target_type_identification.html#performance","title":"Performance","text":"<p>The model was evaluated on the test set of the OLID-BR dataset.</p> <p>Accuracy: 0.7505</p> <p>Precision: 0.7812</p> <p>Recall: 0.7505</p> <p>F1-Score: 0.7603</p> Class Precision Recall F1-Score Support <code>INDIVIDUAL</code> 0.8850 0.7964 0.8384 609 <code>GROUP</code> 0.6766 0.6385 0.6570 213 <code>OTHER</code> 0.4518 0.7177 0.5545 124"},{"location":"models/toxicity_target_type_identification.html#training-procedure","title":"Training procedure","text":""},{"location":"models/toxicity_target_type_identification.html#training-hyperparameters","title":"Training hyperparameters","text":"<p>The following hyperparameters were used during training:</p> <ul> <li>learning_rate: 3.952388499692274e-05</li> <li>train_batch_size: 8</li> <li>eval_batch_size: 8</li> <li>seed: 1993</li> <li>optimizer: Adam with betas=(0.9944095815441554,0.8750000522553327) and epsilon=1.8526084265228802e-07</li> <li>lr_scheduler_type: linear</li> <li>num_epochs: 30</li> </ul>"},{"location":"models/toxicity_target_type_identification.html#framework-versions","title":"Framework versions","text":"<ul> <li>Transformers 4.26.1</li> <li>Pytorch 1.10.2+cu113</li> <li>Datasets 2.9.0</li> <li>Tokenizers 0.13.2</li> </ul>"},{"location":"models/toxicity_target_type_identification.html#provide-feedback","title":"Provide Feedback","text":"<p>If you have any feedback on this model, please open an issue on GitHub.</p>"},{"location":"models/toxicity_type_detection.html","title":"Toxicity Type Detection","text":"<p>Toxicity Type Detection is a model that predicts the type(s) of toxicity(s) in a given text.</p> <p>Toxicity Labels: <code>health</code>, <code>ideology</code>, <code>insult</code>, <code>lgbtqphobia</code>, <code>other_lifestyle</code>, <code>physical_aspects</code>, <code>profanity_obscene</code>, <code>racism</code>, <code>sexism</code>, <code>xenophobia</code></p> <p>This BERT model is a fine-tuned version of neuralmind/bert-base-portuguese-cased on the OLID-BR dataset.</p>"},{"location":"models/toxicity_type_detection.html#overview","title":"Overview","text":"<p>Input: Text in Brazilian Portuguese</p> <p>Output: Multilabel classification (toxicity types)</p>"},{"location":"models/toxicity_type_detection.html#usage","title":"Usage","text":"<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"dougtrajano/toxicity-type-detection\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"dougtrajano/toxicity-type-detection\")\n</code></pre>"},{"location":"models/toxicity_type_detection.html#limitations-and-bias","title":"Limitations and bias","text":"<p>The following factors may degrade the model\u2019s performance.</p> <p>Text Language:  The model was trained on Brazilian Portuguese texts, so it may not work well with Portuguese dialects.</p> <p>Text Origin: The model was trained on texts from social media and a few texts from other sources, so it may not work well on other types of texts.</p>"},{"location":"models/toxicity_type_detection.html#trade-offs","title":"Trade-offs","text":"<p>Sometimes models exhibit performance issues under particular circumstances. In this section, we'll discuss situations in which you might discover that the model performs less than optimally, and should plan accordingly.</p> <p>Text Length: The model was fine-tuned on texts with a word count between 1 and 178 words (average of 18 words). It may give poor results on texts with a word count outside this range.</p>"},{"location":"models/toxicity_type_detection.html#performance","title":"Performance","text":"<p>The model was evaluated on the test set of the OLID-BR dataset.</p> <p>Accuracy: 0.4214</p> <p>Precision: 0.8180</p> <p>Recall: 0.7230</p> <p>F1-Score: 0.7645</p> Label Precision Recall F1-Score Support <code>health</code> 0.3182 0.1795 0.2295 39 <code>ideology</code> 0.6820 0.6842 0.6831 304 <code>insult</code> 0.9689 0.8068 0.8805 1351 <code>lgbtqphobia</code> 0.8182 0.5870 0.6835 92 <code>other_lifestyle</code> 0.4242 0.4118 0.4179 34 <code>physical_aspects</code> 0.4324 0.5783 0.4948 83 <code>profanity_obscene</code> 0.7482 0.7509 0.7496 562 <code>racism</code> 0.4737 0.3913 0.4286 23 <code>sexism</code> 0.5132 0.3391 0.4084 115 <code>xenophobia</code> 0.3333 0.4375 0.3784 32"},{"location":"models/toxicity_type_detection.html#training-procedure","title":"Training procedure","text":""},{"location":"models/toxicity_type_detection.html#training-hyperparameters","title":"Training hyperparameters","text":"<p>The following hyperparameters were used during training:</p> <ul> <li>learning_rate: 7.044186985160909e-05</li> <li>train_batch_size: 8</li> <li>eval_batch_size: 8</li> <li>seed: 1993</li> <li>optimizer: Adam with betas=(0.9339215524915885,0.9916979096990963) and epsilon=3.4435900142455904e-07</li> <li>lr_scheduler_type: linear</li> <li>num_epochs: 30</li> </ul>"},{"location":"models/toxicity_type_detection.html#framework-versions","title":"Framework versions","text":"<ul> <li>Transformers 4.26.0</li> <li>Pytorch 1.10.2+cu113</li> <li>Datasets 2.9.0</li> <li>Tokenizers 0.13.2</li> </ul>"},{"location":"models/toxicity_type_detection.html#provide-feedback","title":"Provide Feedback","text":"<p>If you have any feedback on this model, please open an issue on GitHub.</p>"}]}
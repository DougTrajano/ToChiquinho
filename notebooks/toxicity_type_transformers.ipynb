{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if str(Path(\".\").absolute().parent) not in sys.path:\n",
    "    sys.path.append(str(Path(\".\").absolute().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Initialize the env vars\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.utils import (\n",
    "    compute_pos_weight,\n",
    "    download_dataset,\n",
    "    flatten\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading OLID-BR from Kaggle.\n",
      "Train shape: (4765, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_offensive</th>\n",
       "      <th>is_targeted</th>\n",
       "      <th>targeted_type</th>\n",
       "      <th>toxic_spans</th>\n",
       "      <th>health</th>\n",
       "      <th>ideology</th>\n",
       "      <th>insult</th>\n",
       "      <th>lgbtqphobia</th>\n",
       "      <th>other_lifestyle</th>\n",
       "      <th>physical_aspects</th>\n",
       "      <th>profanity_obscene</th>\n",
       "      <th>racism</th>\n",
       "      <th>religious_intolerance</th>\n",
       "      <th>sexism</th>\n",
       "      <th>xenophobia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>430b13705cf34e13b74bc999425187c3</td>\n",
       "      <td>USER USER é muito bom. USER ^^ E claro a equip...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c779826dc43f460cb18e8429ca443477</td>\n",
       "      <td>Pior do que adolescentezinhas de merda...são p...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e64148caa4474fc79298e01d0dda8f5e</td>\n",
       "      <td>USER Toma no cu é vitamina como tu e tua prima.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "      <td>[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cc66b54eeec24607a67e2259134a1cdd</td>\n",
       "      <td>Muito bom, pena a circunstâncias serem ruins, ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[119, 120, 121, 122, 123, 124, 125, 126, 127, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a3d7839456ae4258a70298fcf637952e</td>\n",
       "      <td>Podia ter beijo também, pra ver se o homofóbic...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  430b13705cf34e13b74bc999425187c3   \n",
       "1  c779826dc43f460cb18e8429ca443477   \n",
       "2  e64148caa4474fc79298e01d0dda8f5e   \n",
       "3  cc66b54eeec24607a67e2259134a1cdd   \n",
       "4  a3d7839456ae4258a70298fcf637952e   \n",
       "\n",
       "                                                text is_offensive is_targeted  \\\n",
       "0  USER USER é muito bom. USER ^^ E claro a equip...          NOT         UNT   \n",
       "1  Pior do que adolescentezinhas de merda...são p...          OFF         UNT   \n",
       "2    USER Toma no cu é vitamina como tu e tua prima.          OFF         TIN   \n",
       "3  Muito bom, pena a circunstâncias serem ruins, ...          OFF         UNT   \n",
       "4  Podia ter beijo também, pra ver se o homofóbic...          OFF         UNT   \n",
       "\n",
       "  targeted_type                                        toxic_spans  health  \\\n",
       "0           NaN                                                NaN   False   \n",
       "1           NaN  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   False   \n",
       "2           GRP  [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17...   False   \n",
       "3           NaN  [119, 120, 121, 122, 123, 124, 125, 126, 127, ...   False   \n",
       "4           NaN  [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...   False   \n",
       "\n",
       "   ideology  insult  lgbtqphobia  other_lifestyle  physical_aspects  \\\n",
       "0     False   False        False            False             False   \n",
       "1     False    True        False            False             False   \n",
       "2     False    True        False            False             False   \n",
       "3     False    True        False            False             False   \n",
       "4     False    True        False            False             False   \n",
       "\n",
       "   profanity_obscene  racism  religious_intolerance  sexism  xenophobia  \n",
       "0              False   False                  False   False       False  \n",
       "1               True   False                  False    True       False  \n",
       "2               True   False                  False   False       False  \n",
       "3              False   False                  False   False       False  \n",
       "4              False   False                  False   False       False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = download_dataset([\"train.csv\", \"test.csv\"])\n",
    "\n",
    "train_df = dataset[\"train.csv\"]\n",
    "test_df = dataset[\"test.csv\"]\n",
    "\n",
    "del dataset\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (1589, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_offensive</th>\n",
       "      <th>is_targeted</th>\n",
       "      <th>targeted_type</th>\n",
       "      <th>toxic_spans</th>\n",
       "      <th>health</th>\n",
       "      <th>ideology</th>\n",
       "      <th>insult</th>\n",
       "      <th>lgbtqphobia</th>\n",
       "      <th>other_lifestyle</th>\n",
       "      <th>physical_aspects</th>\n",
       "      <th>profanity_obscene</th>\n",
       "      <th>racism</th>\n",
       "      <th>religious_intolerance</th>\n",
       "      <th>sexism</th>\n",
       "      <th>xenophobia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>da19df36730945f08df3d09efa354876</td>\n",
       "      <td>USER Adorei o comercial também Jesus. Só achei...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 6...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80f1a8c981864887b13963fed1261acc</td>\n",
       "      <td>Cara isso foi muito babaca geral USER conhece ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2f67025f913e4a6292e3d000d9e2b5a8</td>\n",
       "      <td>Se vc for porco, folgado e relaxado, você não ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>738ccd4476784f47af3a5a6cfdda4695</td>\n",
       "      <td>Se fosse um sniper ia ser louco</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[26, 27, 28, 29, 30]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e0064da693bd4c9e90ce8e6db8bd3bbb</td>\n",
       "      <td>USER é o meu saco USER USER USER</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[13, 14, 15, 16]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  da19df36730945f08df3d09efa354876   \n",
       "1  80f1a8c981864887b13963fed1261acc   \n",
       "2  2f67025f913e4a6292e3d000d9e2b5a8   \n",
       "3  738ccd4476784f47af3a5a6cfdda4695   \n",
       "4  e0064da693bd4c9e90ce8e6db8bd3bbb   \n",
       "\n",
       "                                                text is_offensive is_targeted  \\\n",
       "0  USER Adorei o comercial também Jesus. Só achei...          OFF         UNT   \n",
       "1  Cara isso foi muito babaca geral USER conhece ...          OFF         TIN   \n",
       "2  Se vc for porco, folgado e relaxado, você não ...          OFF         UNT   \n",
       "3                    Se fosse um sniper ia ser louco          OFF         UNT   \n",
       "4                   USER é o meu saco USER USER USER          OFF         UNT   \n",
       "\n",
       "  targeted_type                                        toxic_spans  health  \\\n",
       "0           NaN  [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 6...   False   \n",
       "1           GRP  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   False   \n",
       "2           NaN  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   False   \n",
       "3           NaN                               [26, 27, 28, 29, 30]   False   \n",
       "4           NaN                                   [13, 14, 15, 16]   False   \n",
       "\n",
       "   ideology  insult  lgbtqphobia  other_lifestyle  physical_aspects  \\\n",
       "0     False    True        False            False             False   \n",
       "1     False    True        False            False             False   \n",
       "2     False    True        False            False             False   \n",
       "3     False    True        False            False              True   \n",
       "4     False    True        False            False             False   \n",
       "\n",
       "   profanity_obscene  racism  religious_intolerance  sexism  xenophobia  \n",
       "0               True   False                  False   False       False  \n",
       "1              False   False                  False   False       False  \n",
       "2              False   False                  False   False       False  \n",
       "3              False   False                  False   False       False  \n",
       "4               True   False                  False   False       False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Test shape: {test_df.shape}\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (4272, 16)\n",
      "Test shape: (1438, 16)\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess the dataset.\n",
    "\n",
    "    Args:\n",
    "    - df: The dataset to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    - The preprocessed dataset.\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        \n",
    "        # Filter only offensive comments\n",
    "        df = df[df[\"is_offensive\"] == \"OFF\"]\n",
    "\n",
    "        # Remove religious_intolerance that has only one  sample\n",
    "        if \"religious_intolerance\" in df.columns:\n",
    "            df.drop(\"religious_intolerance\", axis=1, inplace=True)\n",
    "\n",
    "        # Filter only offensive comments with at least one toxicity label\n",
    "        df = df.loc[df.select_dtypes(\"bool\").sum(axis=1).ge(1)]\n",
    "\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "toxicity_labels = [\n",
    "    \"health\",\n",
    "    \"ideology\",\n",
    "    \"insult\",\n",
    "    \"lgbtqphobia\",\n",
    "    \"other_lifestyle\",\n",
    "    \"physical_aspects\",\n",
    "    \"profanity_obscene\",\n",
    "    \"racism\",\n",
    "    \"sexism\",\n",
    "    \"xenophobia\"\n",
    "]\n",
    "\n",
    "train_df = preprocessing(train_df)\n",
    "test_df = preprocessing(test_df)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[\"text\"].values\n",
    "y_train = train_df[toxicity_labels].astype(int).values\n",
    "\n",
    "X_test = test_df[\"text\"].values\n",
    "y_test = test_df[toxicity_labels].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameters(max_seq_length=512, model_name='neuralmind/bert-base-portuguese-cased', model_type='bert', num_train_epochs=1, num_train_epochs_per_child=3, batch_size=4, validation_split=0.2, learning_rate=2e-05, seed=1993)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class Parameters:\n",
    "    max_seq_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model_name: str = field(\n",
    "        default=\"neuralmind/bert-base-portuguese-cased\",\n",
    "        metadata={\n",
    "            \"help\": \"The name of the model to use. It must be a model name or a path to a directory containing model weights.\"\n",
    "        }\n",
    "    )\n",
    "        \n",
    "    model_type: str = field(\n",
    "        default=\"bert\",\n",
    "        metadata={\n",
    "            \"help\": \"The type of the model to use. It must be a model type in the list of available models.\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    num_train_epochs: int = field(\n",
    "        default=1,\n",
    "        metadata={\n",
    "            \"help\": \"The number of epochs to train the model. An epoch is an iteration over the entire training set.\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    num_train_epochs_per_child: int = field(\n",
    "        default=1,\n",
    "        metadata={\n",
    "            \"help\": \"The number of epochs to train the model. An epoch is an iteration over the entire training set.\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    batch_size: int = field(\n",
    "        default=4,\n",
    "        metadata={\n",
    "            \"help\": \"The batch size to use for training and evaluation.\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validation_split: float = field(\n",
    "        default=0.2,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the training set to use as validation set.\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    learning_rate: float = field(\n",
    "        default=2e-5,\n",
    "        metadata={\n",
    "            \"help\": \"The initial learning rate for Adam.\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    seed: int = field(\n",
    "        default=1993,\n",
    "        metadata={\n",
    "            \"help\": \"The seed to use for random number generation.\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "params = Parameters()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert max([len(x.split()) for x in X_train]) <= params.max_seq_length, \"max_seq_length is too small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 12:56:19.119364: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-07 12:56:19.661690: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-07 12:56:19.661733: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-07 12:56:19.751535: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-07 12:56:21.221978: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-07 12:56:21.222130: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-07 12:56:21.222142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-07 12:56:24.805757: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-07 12:56:24.806079: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-07 12:56:24.806110: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (6d53d74e8c97): /proc/driver/nvidia/version does not exist\n",
      "2022-11-07 12:56:24.807673: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier', 'bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Any, Union\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    create_optimizer\n",
    ")\n",
    "    \n",
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    params.model_name,\n",
    "    num_labels=len(toxicity_labels),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label={k:v for k, v in enumerate(toxicity_labels)},\n",
    "    label2id={v:k for k, v in enumerate(toxicity_labels)}\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    params.model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                | 4/5 [00:02<00:00,  1.47ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3417\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 855\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_fn(examples, tokenizer, max_seq_length):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": X_train,\n",
    "        \"labels\": y_train\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    preprocess_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"max_seq_length\": params.max_seq_length\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset = dataset.train_test_split(\n",
    "    test_size=params.validation_split,\n",
    "    shuffle=True,\n",
    "    seed=params.seed\n",
    ")\n",
    "\n",
    "dataset[\"validation\"] = dataset.pop(\"test\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",\n",
    "    max_length=params.max_seq_length,\n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "tf_train_set = dataset[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=params.batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = dataset[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=params.batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(tf_train_set) // params.batch_size\n",
    "total_train_steps = int(batches_per_epoch * params.num_train_epochs)\n",
    "\n",
    "optimizer, lr_scheduler = create_optimizer(\n",
    "    init_lr=params.learning_rate,\n",
    "    num_train_steps=total_train_steps,\n",
    "    num_warmup_steps=0\n",
    ")\n",
    "\n",
    "# Define loss and metrics\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466/855 [===============>..............] - ETA: 1:43:02 - loss: 0.2924 - categorical_accuracy: 0.7028"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    tf_train_set,\n",
    "    validation_data=tf_validation_set,\n",
    "    epochs=params.num_train_epochs,\n",
    "    batch_size=params.batch_size,\n",
    "    # callbacks=[lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToxicityTypeDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Dict, List, Union\n",
    "from sklearn.base import BaseEstimator\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    create_optimizer\n",
    ")\n",
    "\n",
    "\n",
    "def format_scores(scores: Dict[str, Dict[str, float]]) -> Dict[str, float | int]:\n",
    "    \"\"\"Format scores to be logged in mlflow.\n",
    "\n",
    "    Args:\n",
    "    - scores: classification report scores\n",
    "\n",
    "    Returns:\n",
    "    - formatted_scores: formatted scores\n",
    "    \"\"\"\n",
    "    # Flatten the score dict\n",
    "    scores = flatten(scores)\n",
    "\n",
    "    # Remove whitespaces and \"-\" from keys\n",
    "    scores = {k.replace(\" \", \"_\").replace(\"-\", \"_\"): v for k, v in scores.items()}\n",
    "\n",
    "    # Remove \"_support\" items\n",
    "    scores = {k: v for k, v in scores.items() if not k.endswith(\"_support\")}\n",
    "\n",
    "    return scores\n",
    "    \n",
    "\n",
    "class ToxicityTypeDetector(BaseEstimator):\n",
    "    def __init__(self,\n",
    "                 model_type: str = \"bert\",\n",
    "                 model_name: str = \"neuralmind/bert-base-portuguese-cased\",\n",
    "                 labels: List[str] = None,\n",
    "                 max_seq_length: int = 512,\n",
    "                 num_train_epochs: int = 30,\n",
    "                 batch_size: int = 8,\n",
    "                 validation_split: float = 0.2,\n",
    "                 learning_rate: float = 2e-5,\n",
    "                 use_cuda: bool = True,\n",
    "                 **kwargs):\n",
    "\n",
    "        self.model_type = model_type\n",
    "        self.model_name = model_name\n",
    "        self.labels = labels\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.num_train_epochs = num_train_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.learning_rate = learning_rate\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.model = self.init_model()\n",
    "        self.tokenizer = self.init_tokenizer()\n",
    "\n",
    "    def init_tokenizer(self) -> Any:\n",
    "        return BertTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            model_max_length=self.max_seq_length\n",
    "        )\n",
    "\n",
    "    def init_model(self) -> Any:\n",
    "        return TFBertForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=len(self.labels),\n",
    "            problem_type=\"multi_label_classification\",\n",
    "            id2label={k:v for k, v in enumerate(self.labels)},\n",
    "            label2id={v:k for k, v in enumerate(self.labels)}\n",
    "        )\n",
    "\n",
    "    def init_optimizer(self, total_train_steps: int) -> Any:\n",
    "        \"\"\"Initialize the optimizer.\n",
    "\n",
    "        Args:\n",
    "        - total_train_steps: The total number of training steps.\n",
    "\n",
    "        Returns:\n",
    "        - The optimizer.\n",
    "        - The learning rate scheduler.\n",
    "        \"\"\"\n",
    "        return create_optimizer(\n",
    "            init_lr=self.learning_rate,\n",
    "            num_train_steps=total_train_steps,\n",
    "            num_warmup_steps=0\n",
    "        )\n",
    "\n",
    "    def _tokenize(self, X: str) -> Any:\n",
    "        return self.tokenizer.encode_plus(\n",
    "            X,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "\n",
    "    def _predict(self, X: str) -> np.ndarray:\n",
    "        \"\"\"Predicts the toxicity type of a given text.\n",
    "\n",
    "        Args:\n",
    "        - X: The text to be predicted.\n",
    "        \n",
    "        Returns:\n",
    "        - Probabilities of each toxicity type.\n",
    "        \"\"\"\n",
    "        inputs = self._tokenize(X)\n",
    "\n",
    "        logits = self.model(**inputs).logits\n",
    "        probs = tf.nn.sigmoid(logits).numpy()[0]\n",
    "        return probs\n",
    "\n",
    "    def predict_proba(self, X: Union[str, List[str], np.ndarray]) -> List[List[float]]:\n",
    "        \"\"\"Predicts the toxicity types of a given text.\n",
    "\n",
    "        Args:\n",
    "        - X: The text or texts to be predicted.\n",
    "\n",
    "        Returns:\n",
    "        - A list with a dictionary of probabilities for each toxicity type.\n",
    "        \"\"\"\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = X.tolist()\n",
    "        elif isinstance(X, str):\n",
    "            X = [X]\n",
    "\n",
    "        response = []\n",
    "        for x in X:\n",
    "            probs = self._predict(x)\n",
    "            response.append(\n",
    "                {k: v for k, v in zip(self.labels, probs)}\n",
    "            )\n",
    "            \n",
    "        return response\n",
    "\n",
    "    def predict(self, X: Union[str, List[str], np.ndarray]) -> List[List[float]]:\n",
    "        \"\"\"Predicts the toxicity types of a given text.\n",
    "\n",
    "        Args:\n",
    "        - X: The text or texts to be predicted.\n",
    "\n",
    "        Returns:\n",
    "        - A list with a dictionary of predicted toxicity types.\n",
    "        \"\"\"\n",
    "        preds = self.predict_proba(X)\n",
    "        preds = [{k: 1 if v > 0.5 else 0 for k, v in pred.items()} for pred in preds]\n",
    "        return preds\n",
    "\n",
    "    def prepare_data(self, X: np.ndarray, y: Union[list, np.ndarray]) -> Any:\n",
    "        \"\"\"Prepares the data to be used in the model.\n",
    "\n",
    "        Args:\n",
    "        - X: The texts to be used in the model.\n",
    "        - y: The labels to be used in the model.\n",
    "\n",
    "        Returns:\n",
    "        - train_dataset: The train dataset.\n",
    "        - val_dataset: The validation dataset.\n",
    "        \"\"\"\n",
    "        def preprocess_fn(examples, tokenizer):\n",
    "            return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        if isinstance(y, list):\n",
    "            y = np.array(y)\n",
    "\n",
    "        dataset = Dataset.from_dict(\n",
    "            {\n",
    "                \"text\": X,\n",
    "                \"labels\": y\n",
    "            }\n",
    "        )\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            preprocess_fn,\n",
    "            batched=True,\n",
    "            fn_kwargs={\"tokenizer\": self.tokenizer}\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorWithPadding(\n",
    "            tokenizer=self.tokenizer,\n",
    "            return_tensors=\"tf\")\n",
    "\n",
    "        dataset = dataset.train_test_split(\n",
    "            test_size=self.validation_split,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # print(dataset)\n",
    "        \n",
    "        tf_train_set = self.model.prepare_tf_dataset(\n",
    "            dataset[\"train\"],\n",
    "            shuffle=True,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "\n",
    "        tf_validation_set = self.model.prepare_tf_dataset(\n",
    "            dataset[\"test\"],\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "\n",
    "        return tf_train_set, tf_validation_set\n",
    "\n",
    "    def fit(self,\n",
    "            X: List[str],\n",
    "            y: List[List[int]] | np.ndarray) -> None:\n",
    "        \"\"\"Fits the model.\n",
    "\n",
    "        Args:\n",
    "        - X: The texts to be used for training.\n",
    "        - y: The labels to be used for training.\n",
    "        \"\"\"\n",
    "        self.model = self.init_model()\n",
    "        \n",
    "        train_set, val_set = self.prepare_data(X, y)\n",
    "\n",
    "        batches_per_epoch = len(train_set) // self.batch_size\n",
    "        total_train_steps = int(batches_per_epoch * self.num_train_epochs)\n",
    "\n",
    "        optimizer, lr_scheduler = self.init_optimizer(total_train_steps)\n",
    "\n",
    "        # Define loss and metrics\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss,\n",
    "            metrics=[\n",
    "                tf.keras.metrics.BinaryAccuracy()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.model.fit(\n",
    "            train_set,\n",
    "            validation_data=val_set,\n",
    "            epochs=self.num_train_epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            # callbacks=[lr_scheduler]\n",
    "        )\n",
    "\n",
    "    def score(self, X, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = ToxicityTypeDetector(\n",
    "    labels=toxicity_labels,\n",
    "    **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForSequenceClassification\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(params[\"model_name\"])\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    params[\"model_name\"],\n",
    "    num_labels=len(toxicity_labels),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label={k:v for k, v in enumerate(toxicity_labels)},\n",
    "    label2id={v:k for k, v in enumerate(toxicity_labels)}\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\"Olá, meu cachorro é tão fofo\", return_tensors=\"tf\")\n",
    "\n",
    "logits = model(**inputs).logits\n",
    "\n",
    "tf.nn.sigmoid(logits).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

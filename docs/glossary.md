---
title: Toxicity Detection - Glossary
summary: A glossary of terms used in the Toxicity Detection project.
---

Many different terms are used in the literature that we need to know about. This page is a collection of terms definitions that are used in the Toxicity Detection project.

**Hate speech** is a type of speech that is used to attack a group of people.[^2]

**Cyberbullying** is a type of speech that is used to attack an individual directly.[^2]

## Categories

- **Homophobia**: The fear or hatred of homosexuality (and other non-heterosexual identities) and persons perceived to be gay or lesbian.

- **Racism**: Prejudiced thoughts and discriminatory actions based on difference in race/ethnicity.

- **Identity Attack**: Negative or hateful comments targeting someone because of their identity.

- **Insult:** Insulting, inflammatory, or negative comment towards a person or a group of people.

- **Profanity**: Swear words, curse words, or other obscene or profane language.

- **Sexually Explicit**: Contains references to sexual acts, body parts, or other lewd content.

[^1]: Weng, L. (2021, March 21). Reducing toxicity in language models. Lil'Log. https://lilianweng.github.io/lil-log/2021/03/21/reducing-toxicity-in-language-models.html.
[^2]: Zampieri et al. "Predicting the type and target of offensive posts in social media." NAACL 2019.
[^3]: Washington University - Student Affairs. (2020, August 12). Glossary of bias terms. Students. https://students.wustl.edu/glossary-bias-terms/. 